{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora falar um pouco sobre matrizes. Em geral as matrizes são consideradas quase como a coluna dorsal de muitos problemas, então é interessante nos destrincharmos um pouco sobre elas. Inicialmente, podemos nos lembram de uma matriz de rotação de base. Se tivermos um ponto $\\mathbf{r}=(x,y)^T$ e quisermos rotaciona-lo por um angulho $\\theta$, podemos fazer isso da seguinte forma:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "cos \\theta & -sin \\theta \\\\\n",
    "sin \\theta & cos \\theta\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Para o moemnto de inercia tambem, conseguimos definir a matriz de rotação de um corpo rigido ao redor de um eixo arbitario. Por meio do Tensor do momento de inercia, dado por\n",
    "$$\n",
    "I_{\\alpha \\beta} = \\int \\rho \\mathbf{r} \\left(\\delta_{\\alpha \\beta} - r_\\alpha r_\\beta \\right) d^3r\n",
    "$$\n",
    "Onde $\\rho \\mathbf{r}$ é a densidade de massa e $\\alpha, \\beta$ são as componentes cartesianas e $\\delta _{\\alpha \\beta}$ é o denta de Kronecker. O momento de inercia pode ser representado então pela matriz\n",
    "$$\n",
    "\\mathbf{I} = \n",
    "\\begin{bmatrix}\n",
    "I_{xx} & I_{xy} & I_{xz} \\\\\n",
    "I_{yx} & I_{yy} & I_{yz} \\\\\n",
    "I_{zx} & I_{zy} & I_{zz}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Pelo principio da simetria, onde $I_{xy} = I_{yx}$ é possivel achar uma rotação tal que os\n",
    "componentes mistos são igual a zero e nos sobre apenas as componentes diagonais. Tal eixo é chamado de eixo principal e é dado por\n",
    "$$\n",
    "\\mathbf{I}_p = \n",
    "\\begin{bmatrix}\n",
    "I_{xx} & 0 & 0 \\\\\n",
    "0 & I_{yy} & 0 \\\\\n",
    "0 & 0 & I_{zz}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro dos grandes problemas que vamos enfrentar é resolver um sistema de n variaveis com n coeficientes. Em sua forma matricial, pode ser dado como\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Onde podemos simplificar da forma $\\mathbf{Ax = b}$. Vamos colocar que a matriz dos coeficientes, ou seja $\\mathbf{A}$ seja não singular, ou seja, feita de linhas Linearmente Independentes. Vamos tentar resolver esse problema, que inicialmente parece facil. Para começo, vamos usar a matriz dos coeficientes aumentados, que junta as matrizes $\\mathbf{A} \\ e \\ \\mathbf{b}$, de forma:\n",
    "$$\n",
    "(\\mathbf{A} | \\mathbf{b}) =\n",
    "\\left[\n",
    "\\begin{array}{cccc|c}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}& b_1 \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} & b_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn} & b_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro problema que vamos atacar é a forma padrão de matrizes de autovalor, ou seja\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "Onde é equivalente a escrever\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A_{00} & A_{01} & \\cdots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\cdots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\cdots & A_{n-1,n-1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_0 \\\\\n",
    "v_1 \\\\\n",
    "\\vdots \\\\\n",
    "v_{n-1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "v_0 \\\\\n",
    "v_1 \\\\\n",
    "\\vdots \\\\\n",
    "v_{n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Onde $\\lambda$ é o autovalor e $v$ é o autovetor. Uma forma de resolver nosso problema é mudar nossa\n",
    "equação para\n",
    "$$\n",
    "(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0}\n",
    "$$\n",
    "Onde $\\mathbf{I}$ é a matriz identidade. E $\\mathbf{0}$ é uma matriz de zeros. Para uma solução não\n",
    "trivial, vamos precisar que\n",
    "$$\n",
    "\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\n",
    "$$\n",
    "Ou seja, vamos precisar que a matriz $\\mathbf{A} - \\lambda \\mathbf{I}$ seja singular. Expandindo\n",
    "nosso determinante vamos ter nossa equação caracteristica, dada por\n",
    "$$\n",
    "(-1)^n \\lambda^n + c_{n-1} \\lambda^{n-1} + \\cdots + c_1 \\lambda + c_0 = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos introduzir o conceito de norma para uma matriz. Similar a como ja sabemos para um vetor, para\n",
    "uma matriz sua formula é dada por:\n",
    "$$\n",
    "||\\mathbf{A}|| = \\sqrt{\\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} |A_{ij}|^2}\n",
    "$$\n",
    "Para vetores, por sua vez, sua norma é definida como\n",
    "$$\n",
    "||\\mathbf{x}||_E = \\sqrt{\\sum_{i=0}^{n-1} |x_i|^2}\n",
    "$$\n",
    "Vamos agora derivar um criterio para que uma matriz sofra muito de pertubações pequenas, ou seja, quão perto de ser singular ela está. Começando com um problema não pertubado, vamos ter\n",
    "$$\n",
    "\\mathbf{Ax} = \\mathbf{b}\n",
    "$$\n",
    "Agora se adicionarmos uma pertubação pequena $\\mathbf{\\Delta A e \\Delta x}$, vamos ter:\n",
    "$$\n",
    "\\mathbf{A + \\Delta A} \\mathbf{x + \\Delta x} = \\mathbf{b}\n",
    "$$\n",
    "Onde mantemos nossa matriz $\\mathbf{b}$ como constante\n",
    "Expandindo nosso parenteses e substituindo $\\mathbf{b}$, vamos ter\n",
    "$$\n",
    "\\mathbf{A \\Delta x} = -\\mathbf{\\Delta A}(\\mathbf{x + \\Delta x})\n",
    "$$\n",
    "Assumindo $\\mathbf{A}$ como nao singular, vamos ter:\n",
    "$$\n",
    "\\Delta x = -\\mathbf{A}^{-1} \\mathbf{\\Delta A} \\mathbf{x + \\Delta x}\n",
    "$$\n",
    "Tomando a norma em ambos os lados\n",
    "$$\n",
    "||\\Delta x|| = ||\\mathbf{A}^{-1} \\mathbf{\\Delta A} \\mathbf{x + \\Delta x}|| \\leq ||\\mathbf{A}^{-1}||||\\Delta \\mathbf{A}||||\\mathbf{x + \\Delta x}||\n",
    "$$\n",
    "Usando certas propriedades, vamos ter\n",
    "$$\n",
    "\\frac{||\\Delta \\mathbf{x}||}{||\\mathbf{x + \\Delta x}} \\leq ||\\mathbf{A}^{-1}||||\\Delta \\mathbf{A}||\n",
    "$$\n",
    "Dividindo e multiplicando por $\\mathbf{A}$, vamos ter:\n",
    "$$\n",
    "\\frac{||\\Delta \\mathbf{x}||}{||\\mathbf{x + \\Delta x}} \\leq ||\\mathbf{A}|| ||\\mathbf{A}^{-1}||\\frac{||\\Delta \\mathbf{A}||}{||\\mathbf{A}||}\n",
    "$$\n",
    "Com isso, ela nos leva a acharmos o numero de condicionamento de uma matriz, que é dado por:\n",
    "$$\n",
    "\\kappa(\\mathbf{A}) = ||\\mathbf{A}|| ||\\mathbf{A}^{-1}||\n",
    "$$\n",
    "Um alto numero de condicionamento leva a uma matriz que vai sofrer muito por pequenas pertubações, enquanto uma menor sofre menos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora, naturalmente extender para autovalores. Pulando a derivação,nosso criterio de condicionamento para autovalores é dado por:\n",
    "$$\n",
    "\\kappa^{\\lambda_i}_{ev}(\\mathbf{A}) = \\frac{1}{|\\mathbf{u_i^T v_i}|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora tentar resolver equacoes utilizando matrizes. Vamos iniciar para o caso mais simples, que sao para as matrizes triangulares. Matrizes triangulares sao aquelas que na diagonal principal, seja a acima ou abaixo, sao todas zero. Vamos comecar com o uma matriz triangular abaixo $\\mathbf{L}$ de forma \n",
    "$$\n",
    "\\mathbf{Lx = b}\n",
    "$$\n",
    "Logo,\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "L_{00} & 0 & 0 \\\\\n",
    "L_{10} & L{11} & 0 \\\\\n",
    "L_{20} & L_{21} & L_{22}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Onde pode ser expandida na forma\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{00}x_0 &= b_0 \\\\\n",
    "L_{10}x_0 + L_{11}x_1 &= b_1\\\\\n",
    "L_{20}x_0 + L_{21}x_1 + L_{22}x_2 &= b_2\n",
    "\\end{align}\n",
    "$$\n",
    "Podemos derivar um formula geral\n",
    "\n",
    "$$\n",
    "x_i = \\left(b_i - \\sum_{j=0}^{i-1} L_{ij}x_j \\right) \\frac{1}{L_{ii}} , i= 0,1, ..., n-1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma matriz triangular acima $\\mathbf{U}$, nossa derivacao é a mesma nos dando a formula\n",
    "$$\n",
    "x_i = \\left( b_i - \\sum_{j=0}^{i-1} U_{ij}x_j\\right) \\frac{1}{U_{ii}}, i = n-1, n-2, ..., 1, 0\n",
    "$$\n",
    "Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forsub(L, bs):\n",
    "    \"\"\"\n",
    "    L: Matriz triangular superior\n",
    "    bs: coeficientes\n",
    "    \"\"\"\n",
    "    n = bs.size\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        xs[i] = (bs[i] - L[i,:i] @xs[:i])/L[i,i] #o @ é o produto escalar, igual ao .cdot\n",
    "    return xs\n",
    "\n",
    "def backsub(U,bs):\n",
    "    \"\"\"\n",
    "    U: matriz triangular inferior\n",
    "    bs: coeficientes\n",
    "    \"\"\"\n",
    "    n = bs.size\n",
    "    xs = np.zeros(n)\n",
    "    for i in reversed(range(n)):\n",
    "        xs[i] = (bs[i] - U[i, i+1:]@xs[i+1:])/U[i,i]\n",
    "    return xs\n",
    "\n",
    "def testcreate(n,val):\n",
    "    \"\"\"\n",
    "    n: tamanho da matriz\n",
    "    val: valores iniciais\n",
    "    \"\"\"\n",
    "    A = np.arange(val, val+n*n).reshape(n,n)\n",
    "    A = np.sqrt(A)\n",
    "    bs = (A[0,:])**2.1\n",
    "    return A, bs\n",
    "\n",
    "def testsolve(f, A, bs):\n",
    "    \"\"\"\n",
    "    f: funcao de resolver\n",
    "    A: Matriz\n",
    "    bs: coeficientes\n",
    "    \"\"\"\n",
    "    xs = f(A, bs); #print(xs)\n",
    "    xs = np.linalg.solve(A, bs); #print(xs)\n",
    "    \n",
    "A, bs = testcreate(4,21)\n",
    "L = np.tril(A)\n",
    "testsolve(forsub, L, bs)\n",
    "print(\" \")\n",
    "U = np.triu(A)\n",
    "testsolve(backsub, U, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos nos virar agora para o processo de resolver qualquer equacao linear sem a necessidade de nossa matriz ser triangular.Uma das principais maneiras que fazemos isso é por meio da eliminacao gaussiana. A ideia principal é fazermos a eliminacao de termos ate que tenhamos uma matriz triangular superior. Para isso, vamos utilizar do processo de mesclar a matriz $\\mathbf{A}$ com $\\mathbf{B}$. Para um caso geral, com uma matriz $n \\times n$. Inicialmente, dado um passo intermediario, nossa matriz pode ser parecer com\n",
    "$$ %Terminar na ia\n",
    "\\left[\n",
    "\\begin{array}{ccccccccc|c}\n",
    "A_{00} & A_{01} & A_{02} & \\dots & A_{0,j-1} & A_{0, j} & A_{0,j-1} & \\dots & A_{0,n-1} & b_0 \\\\\n",
    "0 & A_{11} & A_{12} & \\dots & A_{1,j-1} & A_{1, j} & A_{1,j-1} & \\dots & A_{1,n-1} & b_1 \\\\\n",
    "0 & 0 & A_{22} & \\dots & A_{2,j-1} & A_{2, j} & A_{2,j-1} & \\dots & A_{2,n-1} & b_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\dots & A_{j-1,j-1} & A_{j-1, j} & A_{j-1,j-1} & \\dots & A_{j-1,n-1} & b_{j-1} \\\\\n",
    "0 & 0 & 0 & \\dots & A_{jj} & A_{jj+1} & A_{j,j+2} & \\dots & A_{j,n-1} & b_j\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\dots & A_{ij} & A_{ij+1} & A_{i,j+2} & \\dots & A_{i,n-1} & b_i \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\dots & A_{n-1,j} & A_{n,j} & A_{n-1,j+1} & \\dots & A_{n-1,n-1} & b_{n-1}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "Onde a linha $j$ virou a linha de pivo, onde todas as linhas antes dela ja foram modificadas, menos a primeira que me mantem igual, e depois dela nenhuma foi modificada, logo $j$ pode variar de\n",
    "$$\n",
    "j = 1, 2, 3 , \\dots, n-2\n",
    "$$\n",
    "Se denotarmos como $i$ linha que esta sendo modificada, seu valor pode variar de\n",
    "$$\n",
    "i = j+1, j+2, \\dots, n-1\n",
    "$$\n",
    "Para a eliminacao, multiplicamos a linha  pivo $j$ por um coeficiente e subtraimos da linha $i$. O coeficiente é escolhido de forma que $i$ comece com 0. Esse coeficiente é escolhido pela equacao\n",
    "$$\n",
    "coef = \\frac{A_{ij}}{A_{jj}}\n",
    "$$\n",
    "Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausselim(inA, inbs):\n",
    "    A = np.copy(inA)\n",
    "    bs = np.copy(inbs)\n",
    "    n = bs.size\n",
    "    \n",
    "    for j in range(n-1):\n",
    "        for i in range(j+1, n):\n",
    "            coeff = A[i,j] / A[j,j]\n",
    "            A[i,j:] -= coeff*A[j,j:]\n",
    "            bs[i] -= coeff*bs[j]\n",
    "    xs = backsub(A,bs)\n",
    "    return xs\n",
    "A, bs = testcreate(4,5)\n",
    "testsolve(gausselim, A, bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse metodo é bastante robusto ate, mas temos aluns problemas. Se tivermos a  mesma matriz $\\mathbf{A}$ mas uma diferente de $\\mathbf{b}$, toda nossa eliminacao foi jogada fora, pois teriamos que chamar denovonossa funcao. Um metodo que resolve isso é metodo LU. Esse metodo vem de varios sabores,mas vamos pegar o mais simples que é por meio de uma decomposicao da nossa matriz $\\mathbf{A}$ em $\\mathbf{LU}$, ou seja, uma triangular inferiore superior. Vamos assumir um item a mais. Nossa matriz $\\mathbf{L}$ é uma matriz inferior unitaria, ou seja,sua diagonal principal contem apenas $1$. Para resolver, vamos descrever nosso sistema como\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{Ax = b}\\\\\n",
    "\\mathbf{LUx = b}\\\\\n",
    "\\mathbf{L (Ux) = b}\\\\\n",
    "\\mathbf{Ly = b}\\\\\n",
    "\\mathbf{Ux = y}\n",
    "\\end{align}\n",
    "$$\n",
    "Vamos implentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_decom(A):\n",
    "    \"\"\"\n",
    "    A: matriz a ser decomposta\n",
    "    Devolve L, U\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    U = np.copy(A)\n",
    "    L= np.identity(n)\n",
    "    \n",
    "    for  j in range(n-1):\n",
    "        for i in range(j+1, n):\n",
    "            coeff = U[i,j] / U[j,j]\n",
    "            U[i,j:] -= coeff*U[j,j:]\n",
    "            L[i,j] = coeff\n",
    "    return L, U\n",
    "\n",
    "def lu_solve(A, bs):\n",
    "    \"\"\"\n",
    "    A: matriz para resolver\n",
    "    bs: matriz dos coeficientes\n",
    "    \"\"\"\n",
    "    L, U = lu_decom(A)\n",
    "    \n",
    "    ys = forsub(L, bs)\n",
    "    xs = backsub(U, ys)\n",
    "    return xs\n",
    "\n",
    "A, bs = testcreate(4,21)\n",
    "testsolve(lu_solve,A,bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversoes de matrizes tambemm sao uma forma de resolvermos. Porem é interessante evitar esse metodo, pelo fato de ser computacionalmente caro. Mas de qualquer jeito é interessante saber. Inicialmente, lembramos do fato\n",
    "$$\n",
    "\\mathbf{\\mathcal{I} = AA^{-1}}\n",
    "$$\n",
    "Se pensarmos de $\\mathcal{I}$ como conposta de varios vetores colunas $e_i$, podemos quebrar nosso problema como $n$ problemas da forma\n",
    "$$\n",
    "\\mathbf{Ax_i = e_i}\n",
    "$$\n",
    "Onde \n",
    "$$\n",
    "\\mathbf{A}^{-1} = \n",
    "\\begin{pmatrix}\n",
    "\\mathbf{x_0} & \\mathbf{x_1} & \\dots & \\mathbf{x_{n-1}} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Utilizando o metodo anterior, podemos  reescrever como\n",
    "$$\n",
    "\\mathbf{L(Ux_i) = e_i}\n",
    "$$\n",
    "esse metodo funciona, mas é computacionalmente muito caro, entao nao utilizamos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos lembrar do determinante. Para uma matriz triangular, seu determinantes é apenas o produto da sua diagoal, logo\n",
    "$$\n",
    "\\det (\\mathbf{A}) = \\det (\\mathbf{L}) + \\det (\\mathbf{U}) = \\left( \\Pi_{i=0}^{n-1} 1 \\right) \\cdot \\left( \\Pi_{i=0}^{n-1} U_{ii}\\right) = \\Pi_{i=0}^{n-i} U_{ii}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resolver alguns problemas do nosso metodo,principalmente se tivermos numeros muito,muito pequenos ou zero como pivo.Para remediar podemos aplicar\n",
    "$$\n",
    "|A_{kj} = \\max_{j \\geq m \\geq n-1} |A_{mj}|\n",
    "$$\n",
    "Onde inves de so pegarmos $A_{jj}$ cegamente, buscamos pelo maior valor e usamos ele. Vamos implementar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_elim_pivot(inA, inbs):\n",
    "    \"\"\"\n",
    "    A: matriz que queremos a solucao\n",
    "    b: matriz dos coeficientes\n",
    "    \"\"\"\n",
    "    A = np.copy(inA)\n",
    "    bs = np.copy(inbs)\n",
    "    n = bs.size\n",
    "    \n",
    "    for j in range(n-1):\n",
    "            k = np.argmax(np.abs(A[j:,j])) + j\n",
    "            if k != j:\n",
    "                A[j,:], A[k,:] = A[k,:], A[j,:].copy()\n",
    "                bs[j] , bs[k] =  bs[k], bs[j]\n",
    "            \n",
    "            for i in range(j+1, n):\n",
    "                coeff = A[i,j] / A[j,j]\n",
    "                A[i,j:] -=coeff*A[j,j:]\n",
    "                bs[i] -= coeff*bs[j]\n",
    "    \n",
    "    xs = backsub(A,bs)\n",
    "    return xs\n",
    "\n",
    "A, bs = testcreate(3,27)\n",
    "testsolve(gauss_elim_pivot, A, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alem desse metodo, que é um metodo,digamos, fechados e diretos, temos metodoso mais interativos, ou seja, aqueles que dinamicamente vao refinando nossa solucao. Um desses que vamos estudar é o metodo iterativo de Jacabi. Nele vamos iniciar com um chute $\\mathbf{x}$ e vamos refinando-a pouco a pouco, com cada iteracao, ate um valor desejado. Com isso, nao é surpreso que eles, geralmente, demoram mais e podem ate nao convergir.\n",
    "Mas vamos pensar, qual a vantagem dele entao se nao da uma solucao exata e demora mais? eles sao mais eficientes na questao computacional e para matrizes dispersas, onde a maioria de seus termos sao $0$, eles sao mais eficientes. Esse metodo é talvez o mais simples e posteriormente veremos outros mais refinados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver seu algoritmo. Reescrevendo nossa equacao por meio de indices\n",
    "$$\n",
    "\\sum_{j=0}^{n-1} A_{ij}x_j = b_i, i=0, 1, 2, \\dots, n-1\n",
    "$$\n",
    "O metodo de jacobi pega essa equacao e tenta resolver para o elemento $x_i$, que corresponde ao elemento da diagonal $A_{ii}$, de forma\n",
    "$$\n",
    "x_i = \\left(b_i - \\sum_{j=0}^{i-1} A_ijx_j - \\sum_{j = i+1}^{n-1}\\right) \\frac{1}{A_{ii}}, i=0, 1, 2, \\dots, n-1\n",
    "$$\n",
    "O metodo comeca chutando um vetor $\\mathbf{x^{(0)}}$ e estimamos o vetor $\\mathbf{x^{(1)}}$ pela formula\n",
    "$$\n",
    "x_i^{(1)} = \\left(b_i - \\sum_{j-0}^{i-1}A_ijx_j^{(0)} - \\sum_{j=i+1}^{n-1} A_ijx_j^{(0)}\\right) \\frac{1}{A_{ii}}, i = 0, 1, 2, \\dots, n-1\n",
    "$$\n",
    "E assim podemos seguir na nossa iteracao ate quando precisarmos. Nossa forma de iteracao se da\n",
    "$$\n",
    "x_i^{(k)} = \\left(b_i - \\sum_{j=0}^{i-1}A_{ij}x_j^{(k-1)} - \\sum_{j=i+1}^{n-1}A_{ij}x_j^{(k-1)}\\right) \\frac{1}{A_{ii}}, i=0, 1, 2, \\dots, n-1\n",
    "$$\n",
    "O chute inicial pode ser arbitrario, talvez uma matriz nula ou apenas com 1. Mas independete do chute a depender da matriz ela nao pode convergir, sendo necessario que elas sejam diagonalmente dominantes. Nem todas sao, mas podem ser rearranjadas para virar. Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termcrit(xolds,xnews):\n",
    "    \"\"\"\n",
    "    Funcao que vai calcular um erro maximo que vamos aceitar para nossa iteracao\n",
    "    xold: vetor com a iteracao passada\n",
    "    xnew: novo x depois da iteracao\n",
    "    \"\"\"\n",
    "    errs = np.abs((xnews-xolds)/xnews)\n",
    "    return np.sum(errs)\n",
    "\n",
    "def jacobi(A, bs, kmax=50, tol=1.e-6):\n",
    "    \"\"\"\n",
    "    Implementacao da nossa interacao de Jacobi\n",
    "    A: matriz inicial que queremos resolver.\n",
    "    bs: matriz de coeficientes\n",
    "    kmax: numero de iteracoes\n",
    "    tol: tolerancia que vamos aceitar\n",
    "    \"\"\"\n",
    "    n = bs.size\n",
    "    xnews = np.zeros(n)\n",
    "    \n",
    "    for k in range(1, kmax):\n",
    "        xs = np.copy(xnews)\n",
    "        for i in range(n):\n",
    "            slt = A[i,:i]@xs[:i]\n",
    "            sgt = A[i, i+1:]@xs[i+1:]\n",
    "            xnews[i] = (bs[i] - slt - sgt) / A[i,i]\n",
    "        \n",
    "        err = termcrit(xs, xnews)\n",
    "        #print(\"it:{}, novo_x:{}, erro:{}\".format(k, xnews, err))\n",
    "        if err < tol:\n",
    "            break\n",
    "    else:\n",
    "        xnews = None\n",
    "    return xnews\n",
    "\n",
    "n = 4; val = 27\n",
    "A, bs = testcreate(n, val)\n",
    "A += val*np.identity(n)\n",
    "testsolve(jacobi, A, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora para a segunda parte bruta da algebra linear, que são resolver problemas com autovalores. Lembramos que nossa equação se da\n",
    "$$\n",
    "\\mathbf{Av_i = \\lambda v_i}\n",
    "$$\n",
    "Vamos supor que nossa matriz $n \\times n$ possui $n$ autovalores distintos. Se isso for verdade, podemos escrever a seguinte relação\n",
    "$$\n",
    "\\mathbf{V^{-1}AV = \\Lambda}\n",
    "$$\n",
    "Onde $\\mathbf{\\Lambda}$ é a matriz diagonal com os autovalores na $\\lambda_i$ nas suas diagonais.\n",
    "$$\n",
    "\\mathbf{\\Lambda} =\n",
    "\\begin{pmatrix}\n",
    "\\lambda_0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Onde $\\mathbf{V}$ é a matriz de vetores colunas que sao os autovetores\n",
    "$$\n",
    "\\mathbf{V} =\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{v_0} & \\mathbf{v_1} & \\dots & \\mathbf{v_{n-1}} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar a atacar nosso problema. Inicialmente, vamos pegar o método mais facil, que é o metodo da potencia. Nele, ele so nos da 1 par autovetor-autovalor. Como cada autovalor é distinto podemos organiza-los de forma\n",
    "$$\n",
    "|\\lambda_0| > |\\lambda_1| > \\dots > |\\lambda_{n-1}|\n",
    "$$\n",
    "O metodo da potencia consiste em pegar um chute inicial $\\mathbf{z^{(0)}}$ e multiplicar pela matro\\ $\\mathbf{A}$ para obtermos a proxima iteracao. Pela formula de chutes, vamos ter\n",
    "$$\n",
    "\\mathbf{z^{(k)} = A^kz^{(0)}} \\quad k=1, 2, 3, \\dots\n",
    "$$\n",
    "Onde podemos expressar nossa matriz $\\mathbf{z^{(0)}}$ como uma combinação linear dos nossos autovetores, da forma\n",
    "$$\n",
    "\\mathbf{z^{(0)}} = \\sum_{i=0}^{n-1} c_i \\mathbf{v_i}\n",
    "$$\n",
    "Onde $c_i$ sao os coeficientes da combinação linear, estes, desconhecidos. Substituindo na nossa formula de iteracao\n",
    "$$\n",
    "\\mathbf{z^{(k)} = A^kz^{(0)}} = \\sum_{i=0}^{n-1} c_i A^k \\mathbf{v_i} = \\sum_{i=0}^{n-1} c_i \\lambda^k \\mathbf{v_i} = c_0 + \\lambda_0^k \\mathbf{v_0} + \\lambda_0^k \\sum_{i=1}^{n-1}c_i \\left(\\frac{\\lambda_i}{\\lambda_0}\\right)^k \\mathbf{v_i} \\quad k=1, 2, 3, \\dots\n",
    "$$\n",
    "Como forma de podermos avaliar essa expressão, vamos introduzir o quociente de Rayleigh dado como\n",
    "$$\n",
    "\\mu (\\mathbf{x}) = \\frac{\\mathbf{x^TAx}}{\\mathbf{x^Tx}}\n",
    "$$\n",
    "Se $\\mathbf{x}$ for um autovetor, temos que $\\mu(\\mathbf{x}) = \\lambda$. Se $\\mathbf{x}$ nao for um autovetor, temos que $\\mu(\\mathbf{x}) \\approx \\lambda$. Como mais um passo, vamos introduzir o vetor $\\mathbf{q^{(k)}}$ que tem a propriedade de sua norma valer $1$, ou seja, ele é a normalização do vetor $\\mathbf{z^{(k)}}$.\n",
    "$$\n",
    "\\mathbf{q^{(k)}} = \\frac{\\mathbf{z^{(k)}}}{||\\mathbf{z^{(k)}}||}\n",
    "$$\n",
    "Logo, nosso quociente de Rayleigh pode ser escrito como\n",
    "$$\n",
    "\\mu(\\mathbf{q^{(k)}}) = [\\mathbf{q}^{(k)}]^T\\mathbf{Aq^{(k)}}\n",
    "$$\n",
    "Logo, nosso algoritmo pode ser escrito como essa sequencia de passos\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z^{(k)}} &= \\mathbf{Aq^{(k-1)}} \\\\\n",
    "\\mathbf{q^{(k)}} &= \\frac{\\mathbf{z^{(k)}}}{||\\mathbf{z^{(k)}}||} \\\\\n",
    "\\mu(\\mathbf{q^{(k)}}) &= [\\mathbf{q^{(k)}}]^T\\mathbf{Aq^{(k)}}\n",
    "\\end{align}\n",
    "$$\n",
    "Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(xs):\n",
    "    \"\"\"Calcula a magnitude do nosso vetor\n",
    "\n",
    "    Args:\n",
    "        xs (array): nosso vetor a qual queremos calcular a magnitude\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(xs*xs))\n",
    "def power(A, kmax=6):\n",
    "    \"\"\"Implementa o algoritimo de potencia\n",
    "\n",
    "    Args:\n",
    "        A (Array): Matriz no qual queremos saber os autovetores e valores\n",
    "        kmax (int, optional): Numero maximo de k. Defaults to 6.\n",
    "    \"\"\"\n",
    "    zs = np.ones(A.shape[0])\n",
    "    qs = zs / magnitude(zs)\n",
    "    for k in range(1, kmax):\n",
    "        zs = A@qs\n",
    "        qs = zs/magnitude(zs)\n",
    "        #print(k, qs)\n",
    "    lam = qs@A@qs\n",
    "    return lam, qs\n",
    "\n",
    "def testeigone(f, A, indx=0):\n",
    "    \"\"\"Vai testar se conseguimos achar nosss autovalores e vetores\n",
    "\n",
    "    Args:\n",
    "        f (Funcao): Funcao que vamos comparar para ver se acertamos\n",
    "        A (Array): Matriz em que vamos achar nosso autovalores e autovetores\n",
    "        indx (int, optional): Index do nosso autovalor e vetor. Defaults to 0.\n",
    "    \"\"\"\n",
    "    eigval, eigvec = f(A)\n",
    "    #print(\" \"); print(eigval); print(eigvec)\n",
    "    npeigvals, npeignvecs = np.linalg.eig(A)\n",
    "    #print(\" \")\n",
    "    #print(npeigvals[indx]); print(npeignvecs[:,indx])\n",
    "\n",
    "A, bs = testcreate(4, 21)     \n",
    "testeigone(power,A)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar uma variação do método da potencia, que nos permitira calcular o autovetor de menor magnitude, inves do de menor. Para isso, vamos usar o metodo da potencia inversa. Ele é muito parecido com o metodo da potencia, mas ao inves de multiplicar pela matriz $\\mathbf{A}$, vamos multiplicar pela matriz inversa $\\mathbf{A^{-1}}$. A formula de iteracao se da\n",
    "$$\n",
    "\\mathbf{Az^{(k)} = q^{(k-1)}} \\quad k=1, 2, 3, \\dots\n",
    "\\mathbf{q^{(k)}} = \\frac{\\mathbf{z^{(k)}}}{||\\mathbf{z^{(k)}}||}\n",
    "\\mu (\\mathbf{q^{(k)}}) = [\\mathbf{q^{(k)}}]^T\\mathbf{q^{(k)}}\n",
    "$$\n",
    "Vamos refinar um pouco esse método. Voltando a definir nossa equação como $\\mathbf{Av_i = \\lambda_i v_i}$, subtrair ambos os lados por $s\\mathbf{v_i}$, onde $s$ é um escalar arbitrario, logo nossa equação se da:\n",
    "$$\n",
    "\\mathbf{(A - s\\mathcal{I})v_i} = \\mathbf{(\\lambda_i - s) v_i}\n",
    "$$\n",
    "Onde pode ser escrita como\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A^{\\star} v_i} &= \\mathbf{\\lambda_i^{\\star} v_i} \\\\\n",
    "\\mathbf{A^{\\star}} &= \\mathbf{A - s\\mathcal{I}} \\\\\n",
    "\\mathbf{\\lambda_i^{\\star}} &= \\mathbf{\\lambda_i - s}\n",
    "\\end{align}\n",
    "$$\n",
    "Aplicando o nosso metodo da potencia inversa, teremos:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A^{\\star}z} &= \\mathbf{q^{(k-1)}} \\\\\n",
    "\\mathbf{q^{(k)}} &= \\frac{\\mathbf{z^{(k)}}}{||\\mathbf{z^{(k)}}||} \\\\\n",
    "\\mu (\\mathbf{q^{(k)}}) &= [\\mathbf{q^{(k)}}]^T\\mathbf{q^{(k)}}\n",
    "\\end{align}\n",
    "$$\n",
    "Onde vamos achar o melhor autovalor mais perto de $s$\n",
    "Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_shift_inv(A, s=20, kmax=200, tol=1e-8):\n",
    "    \"\"\"\n",
    "    A: Matriz que queremos saber menor autovalor\n",
    "    s: valor que vamos dar o shift\n",
    "    kmax: numero maximo de iteracoes\n",
    "    tol: tolerancia\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    znews = np.ones(A.shape[0])\n",
    "    qnews = znews / magnitude(znews)\n",
    "    A_star = A - np.identity(A.shape[0])*s\n",
    "    L, U = lu_decom(A_star)\n",
    "    for k in range(1, kmax):\n",
    "        qs = np.copy(qnews)\n",
    "        ys = forsub(L, qs)\n",
    "        znews = backsub(U, ys)\n",
    "        qnews = znews / magnitude(znews)\n",
    "        \n",
    "        if qs@qnews < 0:\n",
    "            qnews = -qnews\n",
    "            \n",
    "        err = termcrit(qs, qnews)\n",
    "        #print(k, qnews, err)\n",
    "        \n",
    "        if err < tol:\n",
    "            lam = qnews@A@qnews\n",
    "            break\n",
    "    else:\n",
    "        lam = qnews = None\n",
    "    return lam, qnews\n",
    "\n",
    "A, bs = testcreate(4,21)\n",
    "testeigone(power_shift_inv,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos ver outro metodo, chamado decomposicao QR. tal metodo se baseia em decompor nossa matriz em uma diagonal $\\mathbf{Q}$ e outra triangular superior $\\mathbf{R}$, de forma\n",
    "$$\n",
    "\\mathbf{A = QR}\n",
    "$$\n",
    "Sendo esse um metodo direto, diferente dos outros que ja vimos. Pulando a parte da demonstracao, para construirmos nossa matrz ortogonal, com vetores colunas $\\mathbf{q_j}$, vamos fazer\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{a_j ^\\prime} &= \\mathbf{a_j} - \\sum_{i=0}^{j-1} \\mathbf{(q_i^Ta_j)q_i} \\\\\n",
    "\\mathbf{q_j} &= \\frac{\\mathbf{a_j^\\prime}}{||\\mathbf{a_j^\\prime}||}\n",
    "\\end{align}\n",
    "$$\n",
    "Para a matriz $\\mathbf{R}$, construimos de forma\n",
    "$$\n",
    "R_{ij} = \\mathbf{q_i^Ta_j} \\quad j=0,1,\\dots,n-1, \\ i=0,1,\\dots, j-1 \\\\\n",
    "R_{jj} = ||\\mathbf{a_j^\\prime}|| = ||\\mathbf{a_j} - \\sum_{i=0}^{j-1}R_{ij}\\mathbf{q_i}|| \\quad j=0,1,\\dots,n-1\n",
    "$$\n",
    "Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qrdecomposition(A):\n",
    "    \"\"\"\n",
    "    Funcao que faz a nossa decomposicao QR\n",
    "    A: Matriz que vamos decompor\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    Ap = np.copy(A)\n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        for i in range(j):\n",
    "            R[i,j] = Q[:,i] @ A[:,j]\n",
    "            Ap[:,j] -= R[i,j] * Q[:,i]\n",
    "        R[j,j] = magnitude(Ap[:,j])\n",
    "        Q[:,j] = Ap[:,j] / R[j,j]\n",
    "    return Q,R\n",
    "def testqrdecomposition(A):\n",
    "    \"\"\"\n",
    "    vamos testar se nossa decomposicao deu certo ou nao\n",
    "    A: Matriz a ser decomposta\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    Q, R = qrdecomposition(A)\n",
    "    \n",
    "    diff_a = A - Q@R\n",
    "    diff_q = Q.T@Q - np.identity(n)\n",
    "    #print(n,magnitude(diff_a), magnitude(diff_q))\n",
    "for n in range(4,10,2):\n",
    "    A, bs = testcreate(n,21)\n",
    "    testqrdecomposition(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse metodo nos serve bem na questao de nos dar uma boa aproximacao para a matriz $\\mathbf{A}$. Porem, para a matriz $\\mathbf{Q}$, ela nos da uma diferenca muito grande, que chega a ser ate um pouco triste. Vamos introduzir rapidamente o conceito de transformações similares. Dado uma matrzi $\\mathbf{A}$, podemos encontrar uma matriz $\\mathbf{S}$ tal que\n",
    "$$\n",
    "\\mathbf{A^\\prime = S^{-1}AS}\n",
    "$$\n",
    "Onde assumimos apenas que $\\mathbf{S}$ seja invertivel. Com isso, conseguimos demonstrar que\n",
    "$$\n",
    "\\mathbf{A^\\prime S^{-1}v_i} = \\lambda_i \\mathbf{S^{-1}v_i}\n",
    "$$\n",
    "Onde podemos definir que\n",
    "$$\n",
    "\\begin{align}\n",
    "v_i^\\prime &= \\mathbf{S^{-1}v_i}\n",
    "\\mathbf{A^\\prime v_i^prime} &= \\lambda_i \\mathbf{v_i^prime}\n",
    "\\end{align}\n",
    "$$\n",
    "Portanto, descobrimos que duas matrizes similares possuem os mesmos autovalores, mas nao necessariamente os mesmos autovetores. Ademais, caso $\\mathbf{S}$ seja uma matriz ortogonal, podemos dizer que, relembrando que definimos $\\mathbf{Q}$\n",
    "$$\n",
    "\\mathbf{A^\\prime = Q^T A Q}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora discutir um pouco sobre o método de iterações simultâneas. Esse método é a generalização do método das potencias para mais de 1 autovetor. Começando, vamos assumir que todos os autovalores são distintos. Anteriormente, nos começamos com um chute para esse autovalor, mas agora, vamos fazer $n$ chutes numa tentativa de acharmos todos. Logo, nosso vetor inicial se da na forma\n",
    "$$\n",
    "\\mathbf{Z^{(0)}} = \\begin{bmatrix}\n",
    "\\mathbf{z_0^{(0)}} & \\mathbf{z_1^{(0)}} & \\dots & \\mathbf{z_{n-1}^{(0)}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Uma forma de fazermos isso, é colocar que nosso vetor $\\mathbf{Z^{(0)}}$ é uma matriz identidade $n \\times n$ de forma\n",
    "$$\n",
    "\\mathcal{\\mathbf{I}} = \\begin{bmatrix}\n",
    "\\mathbf{e_0} & \\mathbf{e_1} & \\dots & \\mathbf{e_{n-1}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "As nossas próximas matrizes vão se dar na forma\n",
    "$$\n",
    "\\mathbf{Z^{(k)}} = \\mathbf{AZ^{(k-1)}} \\quad k=1,2,3,\\dots\n",
    "$$\n",
    "Um nosso vetor $\\mathbf{z_j^{(k)}}$ se da como\n",
    "$$\n",
    "\\mathbf{z_j^{(k)}} = \\sum_{i=0}^{n-1} c_{ij}\\lambda _i^{(k)}\\mathbf{v_i}\n",
    "$$\n",
    "Precisamos tomar cuidado, que para cada iteração, vamos precisar fazer uma ortonomalização. Portando, nossos passos ficam da forma\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{Z^{(k)}} &= \\mathbf{A}\\mathbf{\\mathbb{Q}^{(k-1)}}\\\\\n",
    "\\mathbf{Z^{(k)}} &= \\mathbf{\\mathbb{Q}^{(k)}}\\mathbf{\\mathbb{R}^{(k)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Onde, $\\mathbf{\\mathbb{Q}^{(0)}} = \\mathcal{\\mathbf{I}}$ Podemos definir $\\mathbf{\\mathbb{R}^{(k)}}$ como o produto em ordem reversa de $\\mathbf{R^{(i)}}$, logo\n",
    "$$\n",
    "\\mathbf{\\mathbb{R}^{(k)}} = \\mathbf{R^{(k)}}\\mathbf{R^{(k-1)}}\\dots\\mathbf{R^{(1)}}\n",
    "$$\n",
    "Portando, disso, pode-se definir que\n",
    "$$\n",
    "\\mathbf{A}^k = \\mathbf{\\mathbb{Q}^{(k)}}\\mathbf{\\mathbb{R}^{(k)}}\n",
    "$$\n",
    "Para uma matriz $\\mathbf{A}$, que é simétrica, essa decomposição nos leva aos autovetores, já para uma matriz $\\mathbf{A}$ que não é simétrica, essa decomposição faz com que $\\mathbf{\\mathbb{Q}^{(k)}}$ convejerá para o \"fator ortogonal\" dos autovetores, ou seja, para a matriz $\\mathbf{\\tilde{Q}}$ em $\\mathbf{V = \\tilde{Q}U}$, onde $\\mathbf{U}$ é uma matriz triangular superior. De qualquer forma $\\mathbf{\\mathbb{Q}^{(k)}}$ esta relacionado com os autovetores de $\\mathbf{A}$.\n",
    "Para extrairmos os autovetores, vamos primeiro generalizar o quociente de Rayleigh, para a forma\n",
    "$$\n",
    "\\mathbf{A^{(k)}} = [\\mathbf{\\mathbb{Q}}^{(k)}]^T\\mathbf{A}\\mathbf{\\mathbb{Q}^{(k)}}\n",
    "$$\n",
    "Portanto, para o método final, vamos ter \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A^{k}} &= [\\mathbf{\\mathbb{Q}^{(k)}}]\\mathbf{\\mathbb{R}^{(k)}}\\\\\n",
    "\\mathbf{A^{(k)}} &= [\\mathbf{\\mathbb{Q}^{(k)}}]^T\\mathbf{A}\\mathbf{\\mathbb{Q}^{(k)}}\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com tudo isso em mente, conseguimos agora definir o algoritmo QR. Ele é muito similar ao das iterações simultâneas, em fato é equivalente. Seus passos, muito simples, são dados por\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A^{(k-1)}} &= \\mathbf{Q^{(k)}}\\mathbf{R^{(k)}}\\\\\n",
    "\\mathbf{A^{(k)}} &= \\mathbf{R^{(k)}}\\mathbf{Q^{(k)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Onde nossa matriz $\\mathbf{A^{(0)}} = \\mathbf{A}$. Olhe que interessantemente, para produzirmos nossa próxima matriz, apenas multiplicamos a nossa da decomposição anterior em ordem reversa. Então, temos que inicialmente decompor nossa matriz $\\mathbf{A}$ em uma $\\mathbf{QR}$, multiplicar em ordem inversa, decompor a proxima e seguir assim. Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "[ 2.13166627e+01 -6.72337148e-02 -6.93022639e-05 -7.50128339e-08]\n"
     ]
    }
   ],
   "source": [
    "def qrmetod(inA, kmax=100):\n",
    "    \"\"\"Implementa o método de solução QR\n",
    "\n",
    "    Args:\n",
    "        inA (Array): Matriz A que queremos os autovalores e vetores\n",
    "        kmax (int, optional): Numero maximo de iterações que vamos fazer. Defaults to 100.\n",
    "    \"\"\"\n",
    "    A = np.copy(inA)\n",
    "    for k in range(1, kmax):\n",
    "        Q, R = qrdecomposition(A)\n",
    "        A = R@Q\n",
    "        #print(k, np.diag(A))\n",
    "    qreigvals = np.diag(A)\n",
    "    return qreigvals\n",
    "\n",
    "A, bs = testcreate(4,21)\n",
    "qreignval = qrmetod(A, 6)\n",
    "print(\" \")\n",
    "npeignvals, npeignvecs = np.linalg.eig(A); print(npeignvals)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queiramos melhorar nosso método, fazendo-o computacionalmente menos caro, podemos fazer um pre-processamento da nossa matriz $\\mathbf{A}$, usando a forma de Hessenberg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que ja temos uma boa estimativa para os autovalores, podemos construir uma boa estimativa para os autovetores. Para isso, vamos usar o método das potencias inversas. Tendo já em mãos os autovalores, esse método converge bastante rápido para os autovetores. O que fazemos foi avaliar nossos autovalores e iterar 1 a 1 pelo método das potencias inversas, nos dando então nosso autovetor. Vamos implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.66974766e+01 -1.74754610e-02 -4.56998776e-06]\n",
      "[ 1.66974766e+01 -1.74754610e-02 -4.56998776e-06]\n",
      " \n",
      "[0.54872345 0.57735058 0.60462293]\n",
      "[-0.54872345 -0.57735058 -0.60462293]\n",
      " \n",
      "[ 0.72582037 -0.0152504  -0.68771522]\n",
      "[-0.72582037  0.0152504   0.68771522]\n",
      " \n",
      "[-0.40155006  0.81649635 -0.41483883]\n",
      "[ 0.40155006 -0.81649635  0.41483883]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def eig(A, eps=1e-12):\n",
    "    \"\"\"Faz o nosso pré processamento e nos acha nossos auto valores e vetores\n",
    "\n",
    "    Args:\n",
    "        A (Array): Matriz A que queremos os autovalores e vetores\n",
    "        eps (float, optional): erro maximo permitido. Defaults to 1e-12.\n",
    "    \"\"\"  \n",
    "    n = A.shape[0]\n",
    "    eigvals = np.zeros(n)\n",
    "    eigvecs = np.zeros((n,n))\n",
    "    qreignvals = qrmetod(A)\n",
    "    for i, qre in enumerate(qreignvals):\n",
    "        eigvals[i], eigvecs[:,i] = power_shift_inv(A, qre+eps) \n",
    "    return eigvals, eigvecs\n",
    "def testeigall(f, A):\n",
    "    \"\"\"Vai testar se conseguimos achar nosss autovalores e vetores\n",
    "\n",
    "    Args:\n",
    "        f (Funcao): Funcao que vamos comparar para ver se acertamos\n",
    "        A (Array): Matriz em que vamos achar nosso autovalores e autovetores\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = f(A)\n",
    "    npeigvals, npeignvecs = np.linalg.eig(A)\n",
    "    print(eigvals); print(npeigvals)\n",
    "    print(\" \")\n",
    "    for eigvecs, npeignvecs in zip(eigvecs.T, npeignvecs.T):\n",
    "        pass\n",
    "        print(eigvecs); print(npeignvecs)\n",
    "        print(\" \")\n",
    "\n",
    "A, bs = testcreate(3, 27)\n",
    "testeigall(eig, A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
